\documentclass[letterpaper]{article}

\usepackage[margin=2.54cm]{geometry}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{url,hyperref}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{assumption}[thm]{Assumption}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}%[section]
\newtheorem{example}[thm]{Example}
\newtheorem{remark}[thm]{Remark}
\newtheorem{algorithm}[thm]{Algorithm}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\Prob}{{\rm Prob}}
\newcommand{\pa}{{\rm pa}}
\newcommand{\nd}{{\rm nd}}
\def\ci{\perp\!\!\!\perp}

\title{Graphical models}
\author{Luis David Garcia-Puente, Sonja Petrovi\'c, Seth Sullivant}
%\date{}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
The {\tt Macaulay2} package {\tt GraphicalModels} contains implementations of basic constructions and algorithms for algebraic statistical models associated to undirected, directed and mixed graphs. 
The basic constructions of conditional independence statements follow classical definitions ...Lauritzen, while the algorithms that manipulate the idelas ... implement results from the work of Sullivant, Garcia-Puente, .. 

{\bf The abstract is not to exceed 150 words.}

{\bf Good example of a recent paper:}\\ \url{http://j-sag.org/Volume4/jsag-1-2012.pdf}
\end{abstract}


\section{Graphical models}
A graphical model is a statistical model associated to a graph,
where the nodes of the graph represent random variables and the
edges in the graph encode relationships between the random variables.
Graphical models are an important class of statistical models used
in many applications (see standard textbooks \cite{Lauritzen, Whitaker})
because of their ability to model complex interactions between
many random variables, by specifying
interactions using only local information about connectivity
between the vertices in a graph.

There are two natural ways to specify a graphical model, through 
either conditional independence statements specified by the graph
or via a parametric representation (sometimes called a ``factorization'').
A fundamental question about a graphical models is: For what
graphs is the parametric representation equivalent to the
representation by conditional independence statements?
Every distribution which factors according to the graph
satisfies the conditional independence statements implied by the 
graph, so one can ask:  Which distributions satisfy the conditional
independence statements implied by the graph, but do not factor?

Once we specify the types of random variables under consideration
(e.g. discrete random variables or Gaussian random variables) it
is possible to address the questions in the preceding paragraph
using (computational) algebraic geometry.  Indeed, in these cases,
the set of all probability distributions satisfying a family of 
conditional independence constraints is a semialgebraic set.
For discrete random variables, that semialgebraic set is a subset
of the probability simplex, and can be represented by a certain
homogeneous ideal generated by quadrics.  For Gaussian random variables,
this set of distributions corresponds to a semialgebraic subset
of the cone of positive definite matrices.  Similarly,
the parametrized family of probability distributions also
is a semialgebraic set (of the probability simplex for discrete
random variables, and of the cone of positive definite matrices
for Gaussian random variables).  This algebraic perspective
has been studied by many authors \cite{}, and the book \cite{DSS}
has many details.

The {\tt Macaulay2} package {\tt GraphicalModels} allows the user to compute
the ideals of conditional independence statements for any collection
of conditional independence statements for discrete or Gaussian
random variables.  It also can compute the vanishing ideal of 
a graphical model in these cases.  A number of auxiliary functions
are useful for doing further analyses of graphical models.

To be explicit, consider the directed acyclic graph $G$ with  five
vertices $\{a,b,c,d,e\}$ and edge
set $\{a \to d, b \to d, c \to d, c \to e, d \to e\}$.
The following commands compute the associated conditional
independence ideal for the set of global Markov statements,
$CI_{{\rm global}(G)}$, and 
the vanishing ideal $I_{G}$ of the gaussian graphical model on $G$.

\begin{verbatim}
Macaulay2, version 1.4

i1 : loadPackage "GraphicalModels";
i2 : G = digraph{{a,d},{b,d},{c,{d,e}},{d,e}} 
o2 = Digraph{a => set {d}   }
             b => set {d}
             c => set {d, e}
             d => set {e}
             e => set {}
i3 : R = gaussianRing G
o3 : PolynomialRing
i4 : vars R
o4 = | s_(a,a) s_(a,b) s_(a,c) s_(a,d) s_(a,e) s_(b,b) s_(b,c) s_(b,d) s_(b,e) s_(c,c) s_(c,d)
     s_(c,e) s_(d,d) s_(d,e) s_(e,e) |
i4 : I = conditionalIndependenceIdeal(R,globalMarkov(G));
i5 : J = gaussianVanishingIdeal(R);
i27 : flatten degrees J
o27 = {1, 1, 1, 2, 3, 3}
\end{verbatim}
 {\tt GraphicalModels} uses the package {\tt Graphs} and a number of fundamental constructs and relationships 
associated with graphs.  First we create a polynomial ring
for carrying out the computations, which contains the entries of
the covariance matrix $\Sigma$ of a jointly normal random vector
as its indeterminates.  Information about the underlying graph is stored with the polynomial ring. 
Hence many functions in the graphical models package use only the ring as input, but require that it be created with {\tt gaussianRing}, or with {\tt markovRing} in the discrete case.

For directed acyclic graphs it is known that 
$V(CI_{{\rm global}(G)}) \cap PD_{m}  =  
V(I_{G}) \cap PD_{m}$,
in particular, the set of positive definite matrices
satisfying the conditional independence constraints equals the set of covariance matrices in the image of the parametrization.
Unfortunately, this does not imply that $CI_{{\rm global}(G)} 
=I_{G}$.  In the case of Gaussian random variables, a larger ideal,
the trek ideal $T_{G}$, generated by all subdeterminants of the 
covariance matrix that vanish on the model, and satisfying
$CI_{{\rm global}(G)} \subseteq T_{G} \subseteq
I_{G}$ is sometimes equal to $I_{G}$, which happens for our
graph of interest $G$ in this case.

\begin{verbatim}
i6 : isSubset(I,J)
o6 = true
i7 : I == J
o7 = false
i9 : J == trekIdeal(R,G)
o9 = true
\end{verbatim}

Similar computations can also be performed for graphical models
with discrete random variables,
and with other graph families.  The mathematical explanation of
these graphical models and their associated ideals
are explained in the remaining sections.


\section{Computing conditional independence ideals}

Conditional independence constraints on discrete or gaussian
random variables translate to rank conditions certain matrices
associated to the probability densities.  We briefly explain
these constructions here and how to generate these constraints
in {\tt Macaulay2} using {\tt GraphicalModels}.  More details
can be obtained in \cite[Ch.~3]{DSS}.

Let $X = (X_{1}, \ldots, X_{n})$ be a discrete random
vector where each random variable $X_{i}$ has state space
$[d_{i}] = \{1,2, \ldots, d_{i} \}$. Let $d = (d_{1}, \ldots, d_{n})$.  
A probability distribution
for $X$ is a tensor in $\mathbb{R}^{d_{1}} \otimes \cdots \otimes \mathbb{R}^{d_{n}}$,
whose coordinates sum to one and all of whose coordinates are nonnegative.
The set of all such distributions is the probability simplex $\Delta_{d}$.
Let $p_{i_{1}\cdots i_{n}} = {\rm P}(X_{1} = i_{1}, \ldots, X_{n} = i_{n})$
denote the probability of a primitive event.  The polynomial
ring in these quantities is created using the command ${\tt markovRing}$.

\begin{verbatim}
i16 : R = markovRing (2,3,2)
o16 : PolynomialRing
i17 : gens R
o17 = {p     , p     , p     , p     , p     , p     , p     , p     , p     ,
        1,1,1   1,1,2   1,2,1   1,2,2   1,3,1   1,3,2   2,1,1   2,1,2   2,2,1 
      p     , p     , p     }
       2,2,2   2,3,1   2,3,2
\end{verbatim}

For $A \subseteq [n]$, let $X_{A} = (x_{a})_{A}$ be the subvector
indexed by $A$.  Let $A,B,C$ be disjoint subsets of $[n]$.  
The conditional independence statement
$X_{A} \ci X_{B} | X_{C}$ holds if and only if the conditional
distribution satisfies
$$
{\rm P}(X_{A} = i_{A}, X_{B} = i_{B} | X_{C} = i_{C}) =
{\rm P}(X_{A} = i_{A} | X_{C} = i_{C})\cdot {\rm P}( X_{B} = i_{B} | X_{C} = i_{C})$$
for all $i_{A}, i_{B}, i_{C}$.   This translates into vanishing
$2\times 2$ minors of certain matrices in the probabilities 
$p_{i_{1}\cdots i_{n}}$.  Those matrices are computed with the 
function {\tt markovMatrices},
and the ideal generated by the $2 \times 2$ minors is computed with 
{\tt conditionalIndependenceIdeal}.  In the following example,
the two conditional independence statements are $X_{1} \ci X_{2} | X_{3}$
and $X_{1} \ci X_{3} \, \,  (:=  X_{1} \ci X_{3} | X_{\emptyset})$.

\begin{verbatim}
i25 : s = {{{1},{2},{3}}, {{1},{3},{}}}
i26 : markovMatrices(R,s)
o26 = {| p_(1,1,1) p_(1,2,1) p_(1,3,1) |, | p_(1,1,2) p_(1,2,2) p_(1,3,2) |, 
       | p_(2,1,1) p_(2,2,1) p_(2,3,1) |  | p_(2,1,2) p_(2,2,2) p_(2,3,2) |  
     | p_(1,1,1)+p_(1,2,1)+p_(1,3,1) p_(1,1,2)+p_(1,2,2)+p_(1,3,2) |}
     | p_(2,1,1)+p_(2,2,1)+p_(2,3,1) p_(2,1,2)+p_(2,2,2)+p_(2,3,2) |
i30 : I = conditionalIndependenceIdeal(R,s);      
\end{verbatim}

There are many different sets of conditional independence statements
associated to a graph $G$, whose nodes correspond to random variables.
For example, the \emph{local Markov statements} of an undirected graph $G$
is the set of conditional independence statements of the form
$X_{i} \ci X_{V \setminus (i \cup N(i)} | X_{N(i)}$
where $N(i)$ is the set of neighbors of $I$ in the graph $G$.
The functions {\tt pairMarkov}, {\tt localMarkov}, and {\tt globalMarkov}
compute the pairwise Markov statements, the local Markov statements, and the global Markov statements for both directed and undirected graphs.

\begin{verbatim}
i44 : G = graph{{1,2},{2,3},{3,4}, {4,5},{1,5}};
i45 : localMarkov G
o45 = {{{1}, {3, 4}, {5, 2}}, {{1, 2}, {4}, {5, 3}}, {{1, 5}, {3}, {4, 2}}, 
      {{2, 3}, {5}, {4, 1}}, {{2}, {4, 5}, {1, 3}}}
\end{verbatim}

For example, the first conditional independence statement produced
is $X_{1} \ci (X_{3}, X_{4}) | (X_{2}, X_{5})$.

A Gaussian random vector $X = (X_{1}, \ldots, X_{n}) 
\sim \mathcal{N}(\mu, \Sigma)$, is an
$n$-dimensional random vector with state space $\mathbb{R}^{n}$ and
density function
$$
f(x)  =  \frac{1}{(2 \pi)^{n/2}(\det \Sigma)^{1/2} } 
\exp\left( - \frac{1}{2} (x- \mu)^{T} \Sigma^{{-1}} (x - \mu) \right)
$$
where $\mu \in \mathbb{R}^{n}$ and $\Sigma = (\sigma_{s,t}) \in PD_{n}$, the cone of 
$n \times n$ symmetric positive definite matrices.  The Gaussian random vector
$X$ satisfies the conditional independence statement $X_{A} \ci X_{B} | X_{C}$
if and only if the submatrix 
$\Sigma_{A \cup C, B \cup C}  := 
(\sigma_{s,t})_{s \in A \cup C, t \in B \cup C}$ has rank $\leq \#C$.
Hence the set of all Gaussian random vectors satisfying a given
collection of conditional independence statements naturally corresponds
to a subset of $PD_{n}$, and are studied algebraically by investigating the
the corresponding determinantal ideals in the polynomial ring
in the $\sigma_{s,t}$ indeterminates.  The corresponding ring
is generated using the command {\tt gaussianRing}, and
computations involving conditional independence ideals with
Gaussian random variables were exemplified in the Introduction.



      



\section{Computing the vanishing ideal of a model}

Graphical models can be described in two possible ways, either by a recursive
factorization of probability distributions or by conditional independence
statements (pairwise, local and global Markov properties). Algebraically, this
corresponds to the principle that varieties can be presented either
parametrically or implicitly. The \emph{vanishing ideal} of a model 
gives the algebraic implicit definition of a graphical model. For discrete
random variables, this ideal is the set 
of homogeneous polynomial relations on the joint probabilities of  the discrete
random variables. For Gaussian models, this ideal is the set  of homogeneous
polynomial relations on the variance-covariance parameters of the model.

The package \textit{GraphicalModels} has the capability of computing the vanishing ideal
of  a \emph{discrete Bayesian network} (discrete graphical model on a directed
graph) and also of a Gaussian graphical model on a directed, undirected, or  mixed
graph (a graph with directed, and bidirected edges). 

The method
\textbf{discreteVanishingIdeal} implements this capability for discrete Bayesian
networks. A Bayesian network $G$ on $n$ discrete random variables 
is defined parametrically by the
\emph{recursive factorization} of the joint probability  distribution
\[p(X) = \prod_{v} p(X_{v} | X_{\pa v}),\]
where the product runs over all vertices $v$ of $G$ and $\pa v$ is the set of
parents of $v$. 
The implementation of this method does not compute the kernel of the
corresponding ring map. Instead, the vanishing ideal is
computed recursively using the factorization 
\[p(X_{1},\dots, X_{n}) = p(X_{1},\dots, X_{n})\cdot p(X_{n} | \pa X_{n}).\]
%At the level of ideals the previous factorization produces the equality
%\[I_{G} = \]

The following example computes the vanishing ideal of the Bayesian network $1\to
2 \to 3\to 4$ on four binary random variables. The vanishing ideal is
minimally generated by 20 quadratic binomials.

\begin{verbatim}
i3 : G = digraph {{1,{2}}, {2,{3}},{3,{4}},{4,{}}};
i4 : R = markovRing (2,2,2,2);
i8 : I = discreteVanishingIdeal (R,G);
o16 : Ideal of R
i37 : betti mingens I

            0  1
o14 = total: 1 20
          0: 1  .
          1: . 20
\end{verbatim}

According to \cite{GSS}, the vanishing ideal of a discrete Bayesian network is the \emph{distinguished
  component} of the conditional independence ideal described by the Markov
statements of the network. For the line graph in our previous example, the
conditional independence ideal for the local Markov statements encoded by $G$
is a radical ideal with 3 associated primes. However, since $G$ is a directed
tree, the conditional independence ideal for the global Markov statements is a
prime ideal and it equals the vanishing ideal of $G$.

 \begin{verbatim}
i15 : J = conditionalIndependenceIdeal (R, localMarkov G);
o16 : Ideal of R
i16 : K = conditionalIndependenceIdeal (R, globalMarkov G);
o21 : Ideal of R
i25 : I == J
o25 = false
i24 : I == K
o24 = true
\end{verbatim}

The method \textit{gaussianVanishingIdeal} computes the vanishing ideal of a
Gaussian graphical model on a graph, digraph, or mixed graph. In the mixed graph
case, the vanishing ideal corresponds to kernel of the polynomial parametrization of the
variance-covariance parameters  $\sigma_{ij}$ in terms of the \emph{direct
  causal effect} parameters $\lambda_{ij}$ associated to the directed edges  
and the parameters $\omega_{ij}$ associated to the
bidirected edges in the mixed graph given by
\[\Sigma = (I - \Lambda)^{-T}\Omega(I-\Lambda)^{-1},\] 
where $\Sigma$ is the variance covariance matrix, $\Lambda$ is the strictly
upper triangular matrix with $\Lambda_{ij} = \lambda_{ij}$ if $i\to j$ is a
directed edge in the model and 0 otherwise; and $\Omega$ is a positive definite
matrix of parameters $\omega_{ij}$ with zeros in each entry $ij$ if there is no
bidirected edge in the model between $i$ and $j$.





\section{Technical discussion (?) }
This package requires Graphs.m2, as a consequence it can do
computations with graphs whose vertices are not necessarily labeled by
integers. 


\section*{Acknowledgements} {\bf TO DO: -- other authors of the package!....} -- Dan Grayson and Amelia Taylor.




\begin{thebibliography}{20}

%\bibitem{theRpackage!}
%Gabriel C. G. de Abreu, Rodrigo Labouriau, David Edwards, High-dimensional Graphical Model Search with gRapHD R Package, \href{http://arxiv.org/abs/0909.1234}{arXiv:0909.1234v4}
%{\bf CITE ME!!!}

\bibitem{DSS} 
M.~Drton, B.~Sturmfels and S.~Sullivant, (2009) \emph{Lectures on Algebraic Statistics}, Birkhauser 

%\bibitem{MortonSturm}
%I. Onur Filiz, Xin Guo, Jason Morton, Bernd Sturmfels, Graphical models for correlated defaults, \href{http://arxiv.org/abs/0809.1393}{arXiv:0809.1393v1}
%{\bf CITE ME!!!}

\bibitem{GSS} L.~D.~Garcia, M.~Stillman and B.~Sturmfels: Algebraic geometry of Bayesian networks, {\em J. Symbolic Comput.}
  {\bf 39} (2005) 331--355.
  
%\bibitem{GeigerMeekSturm}
%Dan Geiger, Christopher Meek, Bernd Sturmfels, On the toric algebra of graphical models, \href{http://arxiv.org/abs/math/0608054}{arXiv:math/0608054v1}
%{\bf CITE ME!!!}

%\bibitem{HaraTakemura}
%Hisayuki Hara, Satoshi Aoki, Akimichi Takemura, Minimal and minimal invariant Markov bases of decomposable models for contingency tables, Bernoulli 2010, Vol. 16, No. 1, 208-233  (preprint FOR OUR REFERENCE: \href{http://arxiv.org/abs/math/0701429}{arXiv:math/0701429v3})
%{\bf CITE ME?}

\bibitem{Lauritzen}
S.~Lauritzen,  (1996) \emph{Graphical models}, Oxford University Press

%\bibitem{Elena}
%E.~Stanghellini, B.~Vantaggi, On the identification of discrete graphical models with hidden nodes, \href{http://arxiv.org/abs/1009.4279}{	arXiv:1009.4279v1}
%{\bf DO WE NEED THIS REFERENCE?}

\bibitem{S} S.~Sullivant: Algebraic geometry of Gaussian Bayesian
  networks, {\em Adv. in Appl. Math.} {\bf 40} (2008) 482--513.

\bibitem{STD}
S.~Sullivant, K.~Talaska, and J.~Draisma: Trek separation for Gaussian
graphical models, {\em Ann. Statist.} {\bf 38} (2010) 1665--1685. 

\bibitem{GPSS} L.~D.~Garcia-Puente, S.~Spielvogel and S.~Sullivant: Identifying
causal effects with computer algebra, to appear in {\em Proceedings of the 26th
Conference of Uncertainty in Artificial Intelligence}.

\bibitem{Whitaker}  Whitaker.  Graphcial Models in Applied Multivariate Statistics.

\end{thebibliography}
\end{document}


